{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "NUM_PROGRAM = 8\n",
    "\n",
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_questions = np.load('cut_Questions.npy')\n",
    "\n",
    "voc_dict = np.load('voc_dict.npy')\n",
    "\n",
    "#print(cut_programs[0][0])\n",
    "sentences = []\n",
    "tag = 0\n",
    "for program in cut_programs:\n",
    "    for episode in program:\n",
    "        for sentence in episode:\n",
    "            sentences.append(TaggedDocument(sentence, [tag]))\n",
    "            tag = tag+1\n",
    "            \n",
    "#print(sentences[:5])    \n",
    "print(len(sentences))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['你', '說', '我們', '做', '父母', '的', '最', '擔心', '的', '就是', '這個'], []]\n"
     ]
    }
   ],
   "source": [
    "print(cut_questions[2][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "#for i in range(1, 7):\n",
    "#    print(cut_questions[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "#model.build_vocab(sentences)\n",
    "#%time model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model = Doc2Vec.load(\"my_doc2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.5853728e-03, -8.4537896e-05, -4.6945019e-03,  2.7422389e-04,\n",
       "       -7.6984981e-04,  2.6734991e-03,  6.5709627e-04,  3.8223609e-03,\n",
       "        4.6199262e-03, -1.6020489e-03,  4.9639405e-03, -4.1916408e-03,\n",
       "        3.1479937e-03,  3.0667556e-03, -4.1953493e-03,  4.2550629e-03,\n",
       "        2.1392393e-03, -1.6338814e-03,  4.2365831e-03,  8.0682233e-04,\n",
       "       -1.3398017e-04, -2.3116970e-03, -4.4043092e-03, -4.6460959e-03,\n",
       "       -4.3832208e-03, -3.3607106e-03, -1.9434276e-03, -1.5336059e-03,\n",
       "        7.1075320e-04, -2.4727514e-04,  1.4304750e-03, -2.2530910e-03,\n",
       "        1.4685007e-03,  3.5515702e-03, -7.1427628e-04,  4.0322328e-03,\n",
       "       -1.1783958e-03, -7.2798772e-05, -2.3685622e-03, -2.1607112e-03,\n",
       "        2.8637899e-03,  1.0442066e-06,  2.9423365e-03,  4.7285338e-03,\n",
       "       -5.3581374e-04, -1.4252778e-03,  4.3024854e-03,  8.4334303e-04,\n",
       "       -2.6899325e-03, -3.4398788e-03,  3.8882000e-03,  1.4797996e-03,\n",
       "        4.0009692e-03, -2.8006509e-03,  1.7065500e-03,  3.5578366e-03,\n",
       "       -5.0661928e-04, -4.9065407e-03,  1.3711595e-04, -1.7225087e-03,\n",
       "       -3.8054236e-03, -2.2267140e-03,  4.2952234e-03, -8.5906562e-04,\n",
       "       -2.1631117e-03, -4.5202277e-03,  1.0838715e-03, -2.4053981e-03,\n",
       "        3.4049333e-03, -3.8863323e-03, -4.0261280e-03, -1.7830794e-03,\n",
       "        1.7496574e-03,  3.3900590e-04, -2.0766824e-03, -1.5365398e-03,\n",
       "       -2.9178690e-03, -1.0679397e-03,  8.9001725e-05, -2.2560884e-03,\n",
       "       -2.6047798e-03,  4.0515349e-03,  1.2072293e-03, -2.7950592e-03,\n",
       "        3.3269287e-03,  4.0017716e-03, -4.2067938e-03, -8.0853055e-04,\n",
       "       -1.3712968e-03, -3.0215010e-03, -3.4593802e-03, -4.2073097e-04,\n",
       "       -1.6241197e-03,  3.6793454e-03,  1.1930234e-03, -3.1845926e-03,\n",
       "        4.7967439e-03, -1.3267152e-03,  3.7197734e-03,  1.7082238e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from gensim.test.utils import get_tmpfile\n",
    "#fname = get_tmpfile(\"my_doc2vec_model\")\n",
    "#model.save(\"my_doc2vec_model\")\n",
    "\n",
    "model.infer_vector(['以後'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_words1 = ['對', ' ', '就是', '這個', ' ', '你', '在', '哪裡', '找到', '它', '的']\n",
    "doc_words2 = ['你', '看', ' ', '這是', '我', '新', '買', '的', '錢包']\n",
    "\n",
    "invec1 = model.infer_vector(doc_words1)\n",
    "invec2 = model.infer_vector(doc_words2)\n",
    "#print(invec1)\n",
    "#print(invec2)\n",
    "\n",
    "# get similarity\n",
    "# the output docid is supposed to be 0\n",
    "sims = model.docvecs.most_similar([invec1])\n",
    "#print(sims)\n",
    "\n",
    "sims = cosine_similarity(invec2.reshape(1, -1), invec2.reshape(1, -1))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#prediction = pd.DataFrame(predictions, columns=['predictions']).to_csv('prediction.csv')\n",
    "\n",
    "out = np.zeros(shape=(500,1))\n",
    "\n",
    "qid=0\n",
    "for question in cut_questions[0]:\n",
    "    qsentence = []\n",
    "    for words in question[0]:\n",
    "        for word in words:\n",
    "            qsentence.append(word)\n",
    "    invec1 = model.infer_vector(qsentence)\n",
    "    idx=0\n",
    "    ans=0\n",
    "    max_score=0\n",
    "    for option in question[1:]:\n",
    "        #print(option)\n",
    "        invec2 = model.infer_vector(option)\n",
    "        #print(invec2)\n",
    "        score = cosine_similarity(invec1.reshape(1, -1), invec2.reshape(1, -1))\n",
    "        #print(score)\n",
    "        if(score > max_score):\n",
    "            ans=idx\n",
    "            max_score=score\n",
    "        idx = idx+1\n",
    "    #print(ans)\n",
    "    #out[qid] = np.array([ans])\n",
    "    qid=qid+1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [1.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [0.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [4.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [5.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [5.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [0.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [0.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [5.]\n",
      " [2.]\n",
      " [4.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [5.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]\n",
      " [5.]\n",
      " [5.]\n",
      " [2.]\n",
      " [5.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [5.]\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]\n",
      " [2.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [0.]\n",
      " [4.]\n",
      " [4.]\n",
      " [5.]\n",
      " [0.]\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [2.]\n",
      " [5.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [0.]\n",
      " [5.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "#print(out)\n",
    "#pd.DataFrame(out, columns=['Answer']).astype(int).to_csv('prediction2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 1000000\n",
    "TRAIN_VALID_RATE = 0.8\n",
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append(model.infer_vector(cut_programs[program_id][episode_id][line_id] + \n",
    "                       cut_programs[program_id][episode_id][line_id+1]))\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append(model.infer_vector(cut_programs[first_program_id][first_episode_id][first_line_id] + \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]))\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys\n",
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "x_train, y_train = Xs[:int(NUM_TRAIN*TRAIN_VALID_RATE)], Ys[:int(NUM_TRAIN*TRAIN_VALID_RATE)]\n",
    "x_valid, y_valid = Xs[int(NUM_TRAIN*TRAIN_VALID_RATE):], Ys[int(NUM_TRAIN*TRAIN_VALID_RATE):]\n",
    "#print(x_train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(x_train, y_train)\n",
    "y_pred = forest.predict(x_valid)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=100.0, random_state=0)\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# testing accuracy\n",
    "y_pred = lr.predict(x_valid)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_valid, y_pred))\n",
    "#print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#prediction = pd.DataFrame(predictions, columns=['predictions']).to_csv('prediction.csv')\n",
    "\n",
    "out = np.zeros(shape=(500,1))\n",
    "\n",
    "\n",
    "qid=0\n",
    "for question in cut_questions:\n",
    "    qsentence = []\n",
    "    for words in question[0]:\n",
    "        for word in words:\n",
    "            qsentence.append(word)\n",
    "    #invec1 = model.infer_vector(qsentence)\n",
    "    idx=0\n",
    "    ans=0\n",
    "    max_score=0\n",
    "    for option in question[1:]:\n",
    "        #print(qsentence)\n",
    "        invec = model.infer_vector((qsentence + option))\n",
    "        #print(invec2)\n",
    "        #score = cosine_similarity(invec1.reshape(1, -1), invec2.reshape(1, -1))\n",
    "        score = lr.predict_proba(invec.reshape(1, -1))\n",
    "        #print(score)\n",
    "        if(score[0][0] > max_score):\n",
    "            ans=idx\n",
    "            max_score=score[0][0]\n",
    "        idx = idx+1\n",
    "    #print(ans)\n",
    "    out[qid] = np.array([ans])\n",
    "    qid=qid+1\n",
    "    \n",
    "pd.DataFrame(out, columns=['Answer']).astype(int).to_csv('prediction3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
