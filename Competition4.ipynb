{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396906\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "NUM_PROGRAM = 8\n",
    "\n",
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_questions = np.load('cut_Questions.npy')\n",
    "\n",
    "voc_dict = np.load('voc_dict.npy')\n",
    "\n",
    "#print(cut_programs[0][0])\n",
    "sentences = []\n",
    "tag = 0\n",
    "for program in cut_programs:\n",
    "    for episode in program:\n",
    "        for sentence in episode:\n",
    "            sentences.append(TaggedDocument(sentence, [tag]))\n",
    "            tag = tag+1\n",
    "            \n",
    "#print(sentences[:5])    \n",
    "print(len(sentences))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['你', '說', '我們', '做', '父母', '的', '最', '擔心', '的', '就是', '這個'], []]\n"
     ]
    }
   ],
   "source": [
    "print(cut_questions[2][0])\n",
    "\n",
    "# 6 optional reponses\n",
    "#for i in range(1, 7):\n",
    "#    print(cut_questions[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t4) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t4) vocabulary scanned & state initialized\n",
      "Training Doc2Vec(dbow,d100,n5,mc2,t4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_list' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_list' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    #Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(sentences)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "    \n",
    "for i in [0,1]:\n",
    "    model = simple_models[i]\n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)\n",
    "    model.save(\"my_doc2vec_simplemodel_%(i)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.370944   -0.21415496 -0.15595432 -0.33281678  0.23018417 -0.2559593\n",
      "  0.29821488  0.54231477  0.03856311  0.642508    0.16359109 -0.40134442\n",
      " -0.24783419  0.57242703 -0.428072   -0.1313937  -0.826978    0.38550678\n",
      "  0.44098067 -0.3575736  -0.04831852  0.37350518  0.25847587 -0.09919845\n",
      " -0.09718899 -0.18083425  0.30289182  0.31137726 -0.12727624 -0.41865405\n",
      "  0.01611029  0.00499394  0.25505286 -0.24586713 -0.386765    0.01128664\n",
      " -0.30906743  0.14207564  0.05504241 -0.26059055  0.8823103  -0.09638795\n",
      " -0.33002147 -0.08997504 -0.32410067  0.43800202  0.00893846  0.42319456\n",
      " -0.02953357  0.3060897 ]\n",
      "[-1.0244675   0.22525074  0.36452842  0.37223774 -0.10813846 -0.08473047\n",
      " -0.6506331   0.08327502  0.1664629  -0.17146659 -0.36690986  0.17627233\n",
      "  0.08673861 -0.15081042 -0.05032846  0.5910078   0.61745477  0.32241046\n",
      "  0.18303536 -0.56708086 -0.13867296  0.03548172 -0.3764001  -0.20676647\n",
      "  0.15308632 -0.12855968  0.23481199  0.11043049 -0.11555359 -0.6955886\n",
      " -0.4286873   0.14627835 -0.21482733  0.16031504  0.6611815   0.356961\n",
      " -0.0063495  -0.17461878 -0.01096585  0.2637053  -0.59633154 -0.5852815\n",
      " -1.2722363  -0.60700196  0.13104148  0.09432893  0.6963036   0.56703216\n",
      " -0.5403196  -0.10028447]\n",
      "[(438787, 0.7165825963020325), (2308098, 0.6996353268623352), (1176616, 0.6881775259971619), (547703, 0.6802314519882202), (1126415, 0.6743366122245789), (1944302, 0.6734728813171387), (1575039, 0.6695796251296997), (2265431, 0.6687654256820679), (162071, 0.6675850749015808), (1955278, 0.6651965379714966)]\n",
      "[[0.9999999]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_words1 = ['對', ' ', '就是', '這個', ' ', '你', '在', '哪裡', '找到', '它', '的']\n",
    "doc_words2 = ['你', '看', ' ', '這是', '我', '新', '買', '的', '錢包']\n",
    "\n",
    "invec1 = model.infer_vector(doc_words1)\n",
    "invec2 = model.infer_vector(doc_words2)\n",
    "print(invec1)\n",
    "print(invec2)\n",
    "\n",
    "# get similarity\n",
    "# the output docid is supposed to be 0\n",
    "sims = model.docvecs.most_similar([invec1])\n",
    "print(sims)\n",
    "\n",
    "sims = cosine_similarity(invec2.reshape(1, -1), invec2.reshape(1, -1))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
